{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20034cc9-b9f7-4101-b4b3-b7e83bc4d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ecommerce risk analysis...\n",
      "Loading and preparing data...\n",
      "Creating advanced features...\n",
      "Calculating risk flags...\n",
      "Performing advanced analytics...\n",
      "Creating visualizations...\n",
      "Generating reports...\n",
      "Analysis complete!\n",
      "Total orders analyzed: 10000\n",
      "Critical risk orders identified: 4249\n",
      "Output files created:\n",
      "- enhanced_ecommerce_data.csv\n",
      "- ecommerce_risk_analysis.png\n",
      "- ecommerce_risk_reports.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "# Configuration\n",
    "RISK_THRESHOLDS = {\n",
    "    'product_return': 0.3,\n",
    "    'category_return': 0.25,\n",
    "    'high_value': 500,\n",
    "    'fast_return_days': 7\n",
    "}\n",
    "\n",
    "# 1. Data Loading and Initial Preparation\n",
    "def load_and_prepare_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Data Cleaning\n",
    "    df['Return_Reason'] = df['Return_Reason'].fillna('Not Returned')\n",
    "    df['Days_to_Return'] = df['Days_to_Return'].fillna(0)\n",
    "\n",
    "    \n",
    "    # Date Handling\n",
    "    df['Order_Date'] = pd.to_datetime(df['Order_Date'], dayfirst=True)\n",
    "    df['Return_Date'] = pd.to_datetime(df['Return_Date'], dayfirst=True, errors='coerce')\n",
    "    \n",
    "    # Data Validation\n",
    "    df = df[(df['Product_Price'] > 0) & (df['Order_Quantity'] > 0)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Feature Engineering\n",
    "def create_features(df):\n",
    "    # Basic Features\n",
    "    df['order_value'] = df['Product_Price'] * df['Order_Quantity']\n",
    "    df['discount_ratio'] = df['Discount_Applied'] / df['Product_Price']\n",
    "    \n",
    "    # Time Features\n",
    "    df['order_day_of_week'] = df['Order_Date'].dt.dayofweek\n",
    "    df['order_month'] = df['Order_Date'].dt.month\n",
    "    df['order_year'] = df['Order_Date'].dt.year\n",
    "    \n",
    "    # Customer Behavior\n",
    "    df['customer_return_rate'] = df.groupby('User_ID')['Return_Status'].transform(\n",
    "        lambda x: (x == 'Returned').mean()\n",
    "    )\n",
    "    df['days_since_last_order'] = df.groupby('User_ID')['Order_Date'].diff().dt.days\n",
    "    \n",
    "    # Product Risk Features\n",
    "    product_stats = df.groupby('Product_ID').agg(\n",
    "        total_orders=('Order_ID', 'count'),\n",
    "        returns=('Return_Status', lambda x: (x == 'Returned').sum())\n",
    "    )\n",
    "    product_stats['product_return_rate'] = product_stats['returns'] / product_stats['total_orders']\n",
    "    \n",
    "    category_stats = df.groupby('Product_Category').agg(\n",
    "        total_orders=('Order_ID', 'count'),\n",
    "        returns=('Return_Status', lambda x: (x == 'Returned').sum())\n",
    "    )\n",
    "    category_stats['category_return_rate'] = category_stats['returns'] / category_stats['total_orders']\n",
    "    \n",
    "    df = df.merge(product_stats['product_return_rate'], on='Product_ID', how='left')\n",
    "    df = df.merge(category_stats['category_return_rate'], on='Product_Category', how='left')\n",
    "    \n",
    "    # Rolling return rates\n",
    "    df = df.sort_values(['Product_ID', 'Order_Date'])\n",
    "    df['rolling_return_rate'] = df.groupby('Product_ID')['Return_Status'].transform(\n",
    "        lambda x: (x == 'Returned').expanding().mean()\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 3. Risk Modeling\n",
    "def calculate_risk_flags(df, thresholds):\n",
    "    # Basic Risk Flags\n",
    "    df['high_risk_product'] = df['product_return_rate'] >= thresholds['product_return']\n",
    "    df['high_risk_category'] = df['category_return_rate'] >= thresholds['category_return']\n",
    "    df['high_value_order'] = df['order_value'] >= thresholds['high_value']\n",
    "    \n",
    "    # Behavioral Risk Flags\n",
    "    df['fast_return'] = (df['Days_to_Return'] <= thresholds['fast_return_days']) & (df['Return_Status'] == 'Returned')\n",
    "    df['defective_return'] = (df['Return_Reason'] == 'Defective') & (df['Return_Status'] == 'Returned')\n",
    "    df['seasonal_risk'] = df['order_month'].isin([11, 12])  # Holiday season\n",
    "    \n",
    "    # Operational Risk Flags\n",
    "    shipping_risk = {\n",
    "        'Next-Day': 0.1,\n",
    "        'Express': 0.15,\n",
    "        'Standard': 0.25\n",
    "    }\n",
    "    df['shipping_risk_score'] = df['Shipping_Method'].map(shipping_risk)\n",
    "    \n",
    "    payment_risk = {\n",
    "        'Credit Card': 0.1,\n",
    "        'Debit Card': 0.15,\n",
    "        'PayPal': 0.08,\n",
    "        'Gift Card': 0.25\n",
    "    }\n",
    "    df['payment_risk_score'] = df['Payment_Method'].map(payment_risk)\n",
    "    \n",
    "    # Composite Risk Score\n",
    "    risk_factors = [\n",
    "        df['high_risk_product'].astype(int),\n",
    "        df['high_risk_category'].astype(int),\n",
    "        df['high_value_order'].astype(int) * 0.5,\n",
    "        df['fast_return'].astype(int),\n",
    "        df['defective_return'].astype(int),\n",
    "        df['shipping_risk_score'],\n",
    "        df['payment_risk_score']\n",
    "    ]\n",
    "    \n",
    "    df['composite_risk_score'] = sum(risk_factors) / (len(risk_factors) - 2)  # Normalize\n",
    "    df['critical_risk'] = df['composite_risk_score'] > 0.5\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4. Advanced Analytics\n",
    "def perform_advanced_analysis(df):\n",
    "    # Product Clustering\n",
    "    product_features = df.groupby('Product_ID').agg({\n",
    "        'Product_Price': 'mean',\n",
    "        'product_return_rate': 'mean',\n",
    "        'order_value': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    product_features['product_cluster'] = kmeans.fit_predict(\n",
    "        product_features[['Product_Price', 'product_return_rate', 'order_value']]\n",
    "    )\n",
    "    \n",
    "    df = df.merge(product_features[['Product_ID', 'product_cluster']], on='Product_ID', how='left')\n",
    "    \n",
    "    # Customer Segmentation\n",
    "    customer_features = df.groupby('User_ID').agg({\n",
    "        'order_value': ['count', 'sum', 'mean'],\n",
    "        'customer_return_rate': 'mean',\n",
    "        'days_since_last_order': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_features.columns = ['User_ID', 'order_count', 'total_spend', 'avg_order_value', \n",
    "                               'return_rate', 'avg_days_between_orders']\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "    customer_features['customer_segment'] = kmeans.fit_predict(\n",
    "        customer_features[['order_count', 'total_spend', 'return_rate']]\n",
    "    )\n",
    "    \n",
    "    df = df.merge(customer_features[['User_ID', 'customer_segment']], on='User_ID', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 5. Visualization\n",
    "def create_visualizations(df):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Return Reasons\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.countplot(y='Return_Reason', \n",
    "                 data=df[df['Return_Status'] == 'Returned'],\n",
    "                 order=df[df['Return_Status'] == 'Returned']['Return_Reason'].value_counts().index)\n",
    "    plt.title('Primary Reasons for Returns')\n",
    "    \n",
    "    # Risk Score Distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(df['composite_risk_score'], bins=20, kde=True)\n",
    "    plt.title('Distribution of Composite Risk Scores')\n",
    "    \n",
    "    # Risk by Category\n",
    "    plt.subplot(2, 2, 3)\n",
    "    risk_by_category = df.groupby('Product_Category')['composite_risk_score'].mean().sort_values()\n",
    "    sns.barplot(x=risk_by_category.values, y=risk_by_category.index)\n",
    "    plt.title('Average Risk Score by Product Category')\n",
    "    \n",
    "    # Payment Method Risk\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.boxplot(x='Payment_Method', y='composite_risk_score', data=df)\n",
    "    plt.title('Risk Distribution by Payment Method')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ecommerce_risk_analysis.png')\n",
    "    plt.close()\n",
    "# 6. Reporting - Corrected Version\n",
    "def generate_reports(df):\n",
    "    # Daily Risk Report\n",
    "    daily_risk = df.groupby(df['Order_Date'].dt.date).agg({\n",
    "        'Order_ID': 'count',\n",
    "        'critical_risk': 'sum',\n",
    "        'order_value': 'sum'\n",
    "    }).rename(columns={\n",
    "        'Order_ID': 'total_orders',\n",
    "        'critical_risk': 'high_risk_orders',\n",
    "        'order_value': 'total_sales'\n",
    "    })\n",
    "    daily_risk['risk_percentage'] = daily_risk['high_risk_orders'] / daily_risk['total_orders']\n",
    "    \n",
    "    # Product Risk Report\n",
    "    product_risk = df.groupby(['Product_ID', 'Product_Category']).agg({\n",
    "        'Order_ID': 'count',\n",
    "        'Return_Status': lambda x: (x == 'Returned').sum(),\n",
    "        'order_value': 'sum',\n",
    "        'composite_risk_score': 'mean'\n",
    "    }).rename(columns={\n",
    "        'Order_ID': 'total_orders',\n",
    "        'Return_Status': 'returns',\n",
    "        'order_value': 'total_sales'\n",
    "    })\n",
    "    product_risk['return_rate'] = product_risk['returns'] / product_risk['total_orders']\n",
    "    \n",
    "    # Save to Excel with formatting using xlsxwriter\n",
    "    with ExcelWriter('ecommerce_risk_reports.xlsx', engine='xlsxwriter') as writer:\n",
    "        df.to_excel(writer, sheet_name='All Orders', index=False)\n",
    "        daily_risk.to_excel(writer, sheet_name='Daily Risk', index=True)\n",
    "        product_risk.to_excel(writer, sheet_name='Product Risk', index=True)\n",
    "        \n",
    "        # Get workbook and worksheet for formatting\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['All Orders']\n",
    "        \n",
    "        # Add conditional formatting for critical risks\n",
    "        red_format = workbook.add_format({'bg_color': '#FFC7CE'})\n",
    "        \n",
    "        # Find the column index of composite_risk_score\n",
    "        risk_col_index = df.columns.get_loc('composite_risk_score')\n",
    "        risk_col_letter = chr(65 + risk_col_index)  # Convert to Excel column letter\n",
    "        \n",
    "        # Apply formatting to all rows in the risk column\n",
    "        worksheet.conditional_format(1, risk_col_index, len(df), risk_col_index, {\n",
    "            'type': 'cell',\n",
    "            'criteria': '>',\n",
    "            'value': 0.5,\n",
    "            'format': red_format\n",
    "        })\n",
    "\n",
    "        # Auto-adjust column widths\n",
    "        for sheet in writer.sheets.values():\n",
    "            sheet.set_column(0, len(df.columns) - 1, 20)\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    print(\"Starting ecommerce risk analysis...\")\n",
    "    \n",
    "    # 1. Load and prepare data\n",
    "    print(\"Loading and preparing data...\")\n",
    "    df = load_and_prepare_data(r'C:\\Users\\TEJASWINI\\Desktop\\ecommerce.csv')\n",
    "    \n",
    "    # 2. Feature engineering\n",
    "    print(\"Creating advanced features...\")\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # 3. Risk modeling\n",
    "    print(\"Calculating risk flags...\")\n",
    "    df = calculate_risk_flags(df, RISK_THRESHOLDS)\n",
    "    \n",
    "    # 4. Advanced analytics\n",
    "    print(\"Performing advanced analytics...\")\n",
    "    df = perform_advanced_analysis(df)\n",
    "    \n",
    "    # 5. Visualization\n",
    "    print(\"Creating visualizations...\")\n",
    "    create_visualizations(df)\n",
    "    \n",
    "    # 6. Reporting\n",
    "    print(\"Generating reports...\")\n",
    "    generate_reports(df)\n",
    "    \n",
    "    # Save final dataset\n",
    "    df.to_csv('enhanced_ecommerce_data.csv', index=False)\n",
    "    \n",
    "    print(\"Analysis complete!\")\n",
    "    print(f\"Total orders analyzed: {len(df)}\")\n",
    "    print(f\"Critical risk orders identified: {df['critical_risk'].sum()}\")\n",
    "    print(f\"Output files created:\")\n",
    "    print(\"- enhanced_ecommerce_data.csv\")\n",
    "    print(\"- ecommerce_risk_analysis.png\")\n",
    "    print(\"- ecommerce_risk_reports.xlsx\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4315ca-7835-4011-8550-64693e713f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5052 high-risk products (return rate ≥ 30.0%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = load_and_prepare_data(r'C:\\Users\\TEJASWINI\\Desktop\\ecommerce.csv')\n",
    "product_stats = df.groupby('Product_ID').agg(\n",
    "    total_orders=('Order_ID', 'count'),\n",
    "    returns=('Return_Status', lambda x: (x == 'Returned').sum())\n",
    ")\n",
    "product_stats['return_rate'] = product_stats['returns'] / product_stats['total_orders']\n",
    "avg_days_to_return = df.groupby('Product_ID')['Days_to_Return'].mean()\n",
    "product_stats = product_stats.join(avg_days_to_return.rename('avg_days_to_return'))\n",
    "avg_discount = df.groupby('Product_ID')['Discount_Applied'].mean()\n",
    "product_stats = product_stats.join(avg_discount.rename('avg_discount'))\n",
    "product_info = df[['Product_ID', 'Product_Category', 'Product_Price']].drop_duplicates()\n",
    "product_stats = product_stats.join(product_info.set_index('Product_ID'))\n",
    "high_risk_threshold = 0.3\n",
    "product_stats['high_risk'] = product_stats['return_rate'] >= high_risk_threshold\n",
    "high_risk_products = product_stats[product_stats['high_risk']].sort_values('return_rate', ascending=False)\n",
    "print(f\"Found {len(high_risk_products)} high-risk products (return rate ≥ {high_risk_threshold*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ceba37-6a06-420a-abe2-47daa5df9436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
